{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The need for a third dataset\n",
    "\n",
    "This notebook illustrates why we need to split our original data into three sets.\n",
    "\n",
    "We use a somewhat peculiar \"hyper-parameter\" to illustrate the point. While you would never try and find \"the best\" random seed in a real world problem, it is a great \"hyper-parameter\" to optimise for us because we know it should not change anything!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format='retina'\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "plt.rcParams[\"font.size\"] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, validation and test\n",
    "\n",
    "Using the breast cancer dataset once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# features are on very different scales so let's\n",
    "# scale them all.\n",
    "X = scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split your data into two datasets. \"trainval\" and \"test\"\n",
    "# The test dataset should be put on a USB drive, locked in\n",
    "# a safe and deleted from your laptop. Only unlock it once\n",
    "# you have frozen *every* parameter and made all choices.\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, random_state=2)\n",
    "\n",
    "# The training and validation dataset is what we will use\n",
    "# day to day to tune our model.\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.978\n",
      "Validation: 0.981\n",
      "Test: 0.986\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3).fit(X_train, y_train)\n",
    "\n",
    "print(\"Train: {:.3f}\".format(knn.score(X_train, y_train)))\n",
    "print(\"Validation: {:.3f}\".format(knn.score(X_val, y_val)))\n",
    "print(\"Test: {:.3f}\".format(knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to make sure, we get roughly the same performance on all three splits. There is nothing special about them.\n",
    "\n",
    "Now we have split the data into three groups, but why? We have a **train**, **validation** and **test** data set.\n",
    "\n",
    "So far we always fit our classifier on the **train** set, and measured performance on the **test** set. Now we split the **train** data into two smaller sets again and call them **train** and **validation**.\n",
    "\n",
    "---\n",
    "\n",
    "For a moment let's pretend there is no **test** set anymore. If you like we could pretend that our train and validation set could be renamed train and test. Keeping the namign straight and\n",
    "communicating it to others takes discipline so I recommend not to rename them.\n",
    "\n",
    "We will fit our model on the **train** dataset, trying different techniques for increasing the accuracy. We know we need to use a different dataset to measure our performance, so we will use the **validation** set to do that. Once we know the technique which has the highest score we will use that for future data.\n",
    "\n",
    "*Note:* This is an example for you to learn from, so we want to use different \"techniques\" that actually all have the same performance. This way we know that any difference we see must be due to random fluctuations.\n",
    "\n",
    "Let's simulate 1000 different attempts to fit a model and select the one with the best validation score. Just like we learnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: 1.000\n"
     ]
    }
   ],
   "source": [
    "val = []\n",
    "test = []\n",
    "for i in range(1000):\n",
    "    rng = np.random.RandomState(i)\n",
    "    # think of this as tuning a hyper-parameter\n",
    "    # a weird parameter but let's just roll with it\n",
    "    noise = rng.normal(scale=.1, size=X_train.shape)\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    # add noise to our dataset and fit the classifier\n",
    "    knn.fit(X_train + noise, y_train)\n",
    "\n",
    "    val.append(knn.score(X_val, y_val))\n",
    "    test.append(knn.score(X_test, y_test))\n",
    "\n",
    "print(\"Validation: {:.3f}\".format(np.max(val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! We managed to get a perfect score on data not used in the training! This \"adding random noise\" trick must be a magical algorithm afterall!\n",
    "\n",
    "Not so fast ...\n",
    "\n",
    "You see that we can overfit the validation set by doing this. Luckily we have the third dataset (the test set) to get an unbiased estimate of the models performance. It checks out as the model fitted on a noisy dataset is no better than any other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 0.986\n"
     ]
    }
   ],
   "source": [
    "print(\"Test: {:.3f}\".format(test[np.argmax(val)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The take away is that if you try enough (random) settings you will find one that performs very well on your test set. However because you look at the test set score while choosing the parameters the score is no longer an unbiased estimate of your algorithms performance on unseen data. However this is the number you and everyone else wants to know!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
