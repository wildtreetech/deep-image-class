{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine-learning, the foundation\n",
    "\n",
    "This notebook introduces the jargon/defines terminology that we will use throughout this course and is in common use in machine-learning circles.\n",
    "\n",
    "Depending on your heritage you might use other words for these concepts. I recommend you try and become familiar with this language (and translate in your head to your words). It makes it easier to communicate with machine-learning people and let's you read articles/blog posts.\n",
    "\n",
    "> A lot of time is spent misunderstanding each other, most often this is because we use the same word but mean different things. Or use different words for the same thing. The machine-learning community is quite precise in their use of language, other fields sometimes are more confused about how they name things.\n",
    "\n",
    "Machine Learning is about building programs with tunable parameters that are adjusted automatically to improve the program's behaviour by adapting to previously seen data.\n",
    "\n",
    "To introduce the jargon and terms we will look at a simple classification problem. Given these red and blue dots we want to learn a model that can tell us what colour a new point is going to be. By learning this separating bouondary we obtain a model that generalises: It works with data that we have never seen before.\n",
    "\n",
    "<img src=\"../images/moons.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a simple dataset to get going. The task is to classify the type of tree (blue or red) based on the coordinates at which the tree is planted.\n",
    "\n",
    "The data will always be organised as a 2D matrix `X` with shape: `n_samples` by `n_features`. Each row is one sample, with each column containing the values for a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# creating our toy dataset\n",
    "labels = [\"b\", \"r\"]\n",
    "X, y = make_blobs(n_samples=400, centers=23, random_state=42)\n",
    "y = np.take(labels, (y < 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features of each sample are stored in the `X` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that we have 400 samples with 2 features each. The measurements for the first sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next ingredient we need is the target, the true value of the type of iris that each sample is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are two different classes:\n",
    "* blue\n",
    "* red\n",
    "\n",
    "Let's plot our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, lw=0, s=40)\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The scikit-learn `Estimator` interface\n",
    "\n",
    "All algorithms in scikit-learn follow the `Estimator` interface. Because of the dominance of scikit-learn a lot of other lirbaries adopt this interface as well. Some libraries are less strict..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The defacto standard interface used for Python based machine-learning\n",
    "class Estimator:\n",
    "    def __init__(self, parameter1):\n",
    "        \"\"\"Construct a new Estimator.\n",
    "        \n",
    "        All hyper-parameters are passed as constructor\n",
    "        arguments.\n",
    "        \"\"\"\n",
    "        self.parameter1 = parameter1\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit estimator to data.\"\"\"\n",
    "        # set state of `self` by learning from `X` (and `y`)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions for `X`.\n",
    "        \n",
    "        An estimator can only make predictions\n",
    "        after it has been `fit()`.\n",
    "        \"\"\"\n",
    "        # returns predictions for `y` at `X`\n",
    "        #\n",
    "        # pass is a Python keyword to signal an empty\n",
    "        # clause, just here to notebook syntactically valid.\n",
    "        pass\n",
    "\n",
    "    def score(self, X, y_true):\n",
    "        \"\"\"Compute score using default scorer of the estimator\"\"\"\n",
    "        # returns a score\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Supervised Learning, we have a dataset consisting of both features and labels. The task is to construct an estimator which is able to predict the label of an object given the set of features. A relatively simple example of this is predicting the type of tree based on its coordinates. However everything here also applies to more complex problems like predicting the name of the person in a colour picture, detecting pedestrians in a street scene, etc.\n",
    "\n",
    "K nearest neighbors (kNN) is one of the simplest learning strategies: given a new, unknown observation, look up in your reference database which ones have the closest features and assign the predominant class. Letâ€™s try it out on our iris classification problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors, datasets\n",
    "\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we received a new observation at `[3., -2.5]`. What would you predict its colour to be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.predict([[3., -2.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.predict([[3., -2.5], [3., +2.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat this exercise for the whole area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_surface(clf, X, y, n_steps=250, subplot=None, show=True,\n",
    "                 ylim=None, xlim=None):\n",
    "    \"\"\"Plot decision surface of `clf`\"\"\"\n",
    "    if subplot is None:\n",
    "        fig = plt.figure()\n",
    "    else:\n",
    "        plt.subplot(*subplot)\n",
    "\n",
    "    if xlim is None:\n",
    "        xlim = X[:, 0].min(), X[:, 0].max()\n",
    "    if ylim is None:\n",
    "        ylim = X[:, 1].min(), X[:, 1].max()\n",
    "    xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], n_steps),\n",
    "                         np.linspace(ylim[0], ylim[1], n_steps))\n",
    "\n",
    "    if hasattr(clf, \"decision_function\"):\n",
    "        z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    else:\n",
    "        z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "    z = z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, z, alpha=0.8, cmap=plt.cm.RdBu_r)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "    plt.xlim(*xlim)\n",
    "    plt.ylim(*ylim)\n",
    "\n",
    "\n",
    "plot_surface(knn, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important take away here is that we learnt the decision rule from the data. If we were asked to work on a new problem we could keep most of the code the same.\n",
    "\n",
    "All we need is a new dataset (with `X` and `y`), learn the decision rule for the new data and we are done. The decision rule would be different, but the way to operate the machinery would remain the same.\n",
    "\n",
    "\n",
    "## Classifying hand written digits\n",
    "\n",
    "Let's demonstrate this by moving to a slightly more complicated dataset. Small images of hand written digits between zero and nine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's exercise our matplotlib skills and make a plot of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(digits.images[42], cmap='gray')\n",
    "print(\"True label:\", digits.target[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each picture is is eight pixels wide and eight pixels high.\n",
    "\n",
    "You can access the targets/labels using `digits.target` and\n",
    "the features for each sample via `digits.data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "* How many features do we have?\n",
    "* How many samples are there?\n",
    "* How many classes are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.predict(X[42:44])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see, everything stayed the same.\n",
    "\n",
    "Next we might want to measure the performance of our algorithm. How many mistakes does it make?\n",
    "\n",
    "## Measuring performance\n",
    "\n",
    "Most estimators have a builtin, default metric that you can access via the `score(X, y_true)` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does it do?\n",
    "#knn.score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What mistake did we just make?\n",
    "\n",
    "Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data.\n",
    "\n",
    "Ultiamtely what we are interested in is making predictions for samples that are from the future!\n",
    "\n",
    "To avoid over-fitting, we have to define two different sets:\n",
    "\n",
    "* a training set `(X_train, y_train)` which is used for learning the parameters of a predictive model\n",
    "* a testing set `(X_test, y_test)` which is used for evaluating the fitted predictive model\n",
    "\n",
    "In scikit-learn such a random split can be quickly computed with the `train_test_split()` function.\n",
    "\n",
    "\n",
    "## Performance on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the data into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't a very difficult problem, so we get a very good score without much work.\n",
    "\n",
    "You can also get a classification report that gives some more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.classification_report(y_test, knn.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Use the `LogisticRegression` algorithm to create a classifier that can differentiate benign from malignant breast cancer tumors.\n",
    "\n",
    "Answer the following questions:\n",
    "* how many features does each sample have?\n",
    "* how many samples are in this dataset?\n",
    "* how many different classes are there?\n",
    "* what is your estimate of the generalisation error, or said differently: what accuracy do you predict this classifier will achieve on unseen data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "A summary of the important terms:\n",
    "\n",
    "* samples: each set of measurements is a sample. For example a row of a database\n",
    "* features: each property of a sample is a feature\n",
    "* labels: the label/category we want to predict for each sample\n",
    "* supervised learning: we have a dataset `X` that is a 2D matrix with `n_samples` rows and `n_features` columns, and a set of true labels for each sample stored in `y`\n",
    "* our goal is to learn parameters from the dataset that generalise, this allows us to make predictions for examples we have not yet seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
