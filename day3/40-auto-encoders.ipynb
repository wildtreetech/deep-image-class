{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto encoders\n",
    "\n",
    "This notebook introduces the concept of auto-encoders. These are neural networks that output their input. This does not seem overaly useful at first, however the network contains a bottle neck. This means that the network can not just learn the identity (copy inputs to outputs) but needs to find a way to represent the inputs using much less information and then recreate the original inputs from that representation.\n",
    "\n",
    "This is a good example of neural networks being feature transformers. They transform your images from a pixel representation to a smaller one that represents the same information. Similar maybe to a compression algorithm.\n",
    "\n",
    "This notebook is based on the examples in https://blog.keras.io/building-autoencoders-in-keras.html. Follow along there as well for ideas on things to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format='retina'\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "from sklearn.utils import check_random_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-encoders\n",
    "\n",
    "Create a simple Auto Encoder using `keras`. By forcing the input through a bottleneck we can hope that the auto encoder focusses on interesting structure in the data and ignores noise. Maybe auto encoders can be used to perform denoising.\n",
    "\n",
    "* create your own auto encoder setup. You might have to construct an auto encoder that has more layers or uses more powerful transformations like `Conv2D`, `MaxPooling2D` and `Upsampling2D`. Start with a simple one first for debugging though.\n",
    "* make sure to load the MNIST dataset from `keras` and not the digits dataset from scikit-learn.\n",
    "* Create a noisy version of your digits by drawing pixel values from a gaussian with mean equal to each pixel's intensity and a small standard deviation (tip: start with little/no noise to debug your auto encoder and then slowly increase the noise)\n",
    "\n",
    "(Bonus: can you learn an \"auto encoder\" that rotates clean digits by 90degrees? This shuold start you off thinking about using neural networks as general purpose \"transformers\".)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "# note: the MNIST dataset has more features and more samples than the digits\n",
    "# dataset in scikit-learn\n",
    "# Only load the training images, we don't need more and we do not care for the labels\n",
    "(x_train, y_train), (_, _) = mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 28*28)\n",
    "\n",
    "X = x_train / 255\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_train,\n",
    "                                                    train_size=0.8, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an extremely simple auto-encoder. It has just one fully connected layer. It goes from 782 dimensions to 32 in the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# go from 784 dimensions to 32, a reduction of factor 24\n",
    "encoding_dim = 32\n",
    "\n",
    "input_img = Input(shape=(784,))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "decoded = Dense(784, activation='relu')(encoded)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model = autoencoder.fit(X_train, X_train,\n",
    "                        epochs=50,\n",
    "                        batch_size=256,\n",
    "                        shuffle=True,\n",
    "                        verbose=False,\n",
    "                        validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shortcut to perform just the encoding\n",
    "encoder = Model(input_img, encoded)\n",
    "\n",
    "# shortcut to perform just the decoding step\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "X_encoded = encoder.predict(X_test)\n",
    "X_decoded = decoder.predict(X_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(original, reconstructed, n=10):\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n):\n",
    "        # original\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(original[i].reshape(28, 28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # reconstruction\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(reconstructed[i].reshape(28, 28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        \n",
    "plot_comparison(X_test, X_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_noisy = X_train + 0.05 * np.random.normal(loc=0.0, scale=1., size=X_train.shape)\n",
    "X_test_noisy = X_test + 0.05 * np.random.normal(loc=0.0, scale=1., size=X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(784,))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "decoded = Dense(784, activation='relu')(encoded)\n",
    "\n",
    "denoise_autoencoder = Model(input_img, decoded)\n",
    "\n",
    "denoise_autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "denoise_model = denoise_autoencoder.fit(X_train_noisy, X_train,\n",
    "                                        epochs=50,\n",
    "                                        batch_size=256,\n",
    "                                        shuffle=True,\n",
    "                                        verbose=False,\n",
    "                                        validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shortcut to perform just the encoding\n",
    "denoise_encoder = Model(input_img, encoded)\n",
    "\n",
    "# shortcut to perform just the decoding step\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "decoder_layer = denoise_autoencoder.layers[-1]\n",
    "denoise_decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "X_encoded_noisy = denoise_encoder.predict(X_test_noisy)\n",
    "denoise_X_decoded = denoise_decoder.predict(X_encoded_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison(X_test_noisy, denoise_X_decoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
