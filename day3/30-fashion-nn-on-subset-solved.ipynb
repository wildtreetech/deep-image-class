{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained networks for fashion\n",
    "\n",
    "The fashion MNIST dataset is pretty big, but what if we only had a few 10s of examples from each category?\n",
    "\n",
    "In this notebook and the next we will illustrate the benefit of pretraining your network on a very large dataset that is only somewhat related to the task you want to solve. In our case we will pretrain on MNIST digits to help solve the fashion MNIST dataset.\n",
    "\n",
    "Here we train a small fully connected network using only 300 examples. This is to simulate what it is like to only have a small dataset.\n",
    "\n",
    "After this in the next notebook we will pretrain a more complex model on MNIST digits, and then use that trained network for fashion MNIST with only 300 examples. Hopefully we get better performance than here.\n",
    "\n",
    "In a real world setting you would use a network pretrained on ImageNet or similar and apply it to your use case (say rock, paper, scissors). However training on ImageNet still takes too long for a classroom setting so we have to make do with these smaller toy sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras import utils\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
    "                                                  test_size=10000,\n",
    "                                                  random_state=42)\n",
    "# performing the split in two steps like this makes sure our validation\n",
    "# dataset is the same in both cases. Just to make sure we don't suffer from\n",
    "# statistical uncertainty when comparing validation accuracies\n",
    "X_train = X_train[:300]\n",
    "y_train = y_train[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_val = utils.to_categorical(y_val, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully connected network from scratch\n",
    "\n",
    "Let's see how well it performs.\n",
    "\n",
    "Question: compare the performance to what you get when you use the full training set (Maybe with a more powerful model -> more 'neurons' per layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Activation, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "x = Input(shape=(28, 28, ))\n",
    "\n",
    "h = Flatten()(x)\n",
    "\n",
    "h = Dense(20)(h)\n",
    "h = Activation('relu')(h)\n",
    "h = Dense(20)(h)\n",
    "h = Activation('relu')(h)\n",
    "h = Dense(10)(h)\n",
    "y = Activation('softmax')(h)\n",
    "\n",
    "net = Model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.compile(loss='categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 300 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "300/300 [==============================] - 0s 2ms/step - loss: 2.2678 - acc: 0.1233 - val_loss: 2.1697 - val_acc: 0.1427\n",
      "Epoch 2/20\n",
      "300/300 [==============================] - 0s 487us/step - loss: 2.0809 - acc: 0.2500 - val_loss: 2.0407 - val_acc: 0.3445\n",
      "Epoch 3/20\n",
      "300/300 [==============================] - 0s 500us/step - loss: 1.9577 - acc: 0.3400 - val_loss: 1.9488 - val_acc: 0.3502\n",
      "Epoch 4/20\n",
      "300/300 [==============================] - 0s 496us/step - loss: 1.8572 - acc: 0.3533 - val_loss: 1.8633 - val_acc: 0.3846\n",
      "Epoch 5/20\n",
      "300/300 [==============================] - 0s 491us/step - loss: 1.7540 - acc: 0.4367 - val_loss: 1.7629 - val_acc: 0.4744\n",
      "Epoch 6/20\n",
      "300/300 [==============================] - 0s 484us/step - loss: 1.6401 - acc: 0.4900 - val_loss: 1.6699 - val_acc: 0.4650\n",
      "Epoch 7/20\n",
      "300/300 [==============================] - 0s 486us/step - loss: 1.5360 - acc: 0.4933 - val_loss: 1.5811 - val_acc: 0.4849\n",
      "Epoch 8/20\n",
      "300/300 [==============================] - 0s 478us/step - loss: 1.4353 - acc: 0.5200 - val_loss: 1.4970 - val_acc: 0.4853\n",
      "Epoch 9/20\n",
      "300/300 [==============================] - 0s 482us/step - loss: 1.3422 - acc: 0.5467 - val_loss: 1.4215 - val_acc: 0.5170\n",
      "Epoch 10/20\n",
      "300/300 [==============================] - 0s 491us/step - loss: 1.2549 - acc: 0.5833 - val_loss: 1.3499 - val_acc: 0.5664\n",
      "Epoch 11/20\n",
      "300/300 [==============================] - 0s 504us/step - loss: 1.1706 - acc: 0.6367 - val_loss: 1.2855 - val_acc: 0.5968\n",
      "Epoch 12/20\n",
      "300/300 [==============================] - 0s 524us/step - loss: 1.0914 - acc: 0.6767 - val_loss: 1.2185 - val_acc: 0.6217\n",
      "Epoch 13/20\n",
      "300/300 [==============================] - 0s 540us/step - loss: 1.0166 - acc: 0.6900 - val_loss: 1.1637 - val_acc: 0.6311\n",
      "Epoch 14/20\n",
      "300/300 [==============================] - 0s 520us/step - loss: 0.9456 - acc: 0.7133 - val_loss: 1.1079 - val_acc: 0.6408\n",
      "Epoch 15/20\n",
      "300/300 [==============================] - 0s 502us/step - loss: 0.8786 - acc: 0.7367 - val_loss: 1.0569 - val_acc: 0.6487\n",
      "Epoch 16/20\n",
      "300/300 [==============================] - 0s 516us/step - loss: 0.8297 - acc: 0.7433 - val_loss: 1.0162 - val_acc: 0.6534\n",
      "Epoch 17/20\n",
      "300/300 [==============================] - 0s 529us/step - loss: 0.7804 - acc: 0.7567 - val_loss: 0.9866 - val_acc: 0.6543\n",
      "Epoch 18/20\n",
      "300/300 [==============================] - 0s 561us/step - loss: 0.7359 - acc: 0.7700 - val_loss: 0.9514 - val_acc: 0.6669\n",
      "Epoch 19/20\n",
      "300/300 [==============================] - 0s 550us/step - loss: 0.6999 - acc: 0.7733 - val_loss: 0.9319 - val_acc: 0.6774\n",
      "Epoch 20/20\n",
      "300/300 [==============================] - 0s 578us/step - loss: 0.6667 - acc: 0.7867 - val_loss: 0.9039 - val_acc: 0.6773\n"
     ]
    }
   ],
   "source": [
    "history = net.fit(X_train, y_train,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=20,\n",
    "                  verbose=1,\n",
    "                  validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
